{"uv_id": "AI18", "resource_id": "R001", "fragment_id": "F001", "fragment_type": "section", "section_title": "Prototyping activity", "text": "The purpose of this activity is to learn more about low-fidelity prototyping by creating a simple, hand-drawn prototype in less than 5 minutes, and simulating it with another user. Divide up into pairs for this activity. Prototype Your Alarm Clock Think about the alarm clock you use to wake up every day. It may be a digital clock radio, it may be an analog clock, it may even be a cellphone or a desktop application. Make a low-fidelity prototype of your alarm clock. Include enough of the interface so that your low-fi prototype can display and change the current time, display and change the alarm time, and turn the alarm on and off. Each of you should draw your own alarm clock at the same time. Don’t discuss it with your partner yet. Run Your Prototype Simulate your prototype, acting as the Computer, while your partner acts as user. Use these tasks: Is the alarm set to wake me up at 9 am? Suppose not. Set the alarm to wake me up at 9 am. Set the current time one hour backward for a daylight savings time switch. Then switch roles, so that the other person acts as the Computer simulating their own prototype on you.", "resource_start_char": 0, "resource_end_char": 1128, "token_count": 206, "candidates": ["GI_C1_3", "GI_C1_1", "GI_C2_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 65, "end": 76, "quote": "prototyping"}}]}
{"uv_id": "AI18", "resource_id": "R002", "fragment_id": "F002", "fragment_type": "section", "section_title": "Accessibility activity", "text": "The purpose of this activity is to experience what it’s like to use a web page with screen reader software. Go to WebAnywhere, a free, public screen reader implemented as a web proxy. Look over the instructions (particularly the keyboard shortcuts!) and play with it a bit on WebAnywhere’s home page. Then use WebAnywhere to visit this page (note that you won’t see anything, but you’ll hear it), and try to answer these questions: 1. What page is this? 2. Who is profiled today? 3. What is today’s spotlight? Assuming you figured out the answer to #1, now go to the real web page and see how easy it is to answer these questions visually by comparison. Other Resources: WebAIM has created simulations of accessibility problems. Mac OS X has a built-in screen reader called VoiceOver. For Windows, the screen readers JAWS and Window Eyes have trial versions you can install. ADesigner helps check the degree of accessibility of a web page to a screen reader.", "resource_start_char": 0, "resource_end_char": 958, "token_count": 166, "candidates": ["GI_C1_3", "GI_C1_1", "GI_C2_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 705, "end": 718, "quote": "accessibility"}}]}
{"uv_id": "AI18", "resource_id": "R003", "fragment_id": "F003", "fragment_type": "section", "section_title": "Course description", "text": "Students explore augmented reality audio through the design and evaluation of prototypes. Participants will probe design space and illuminate creative possibilities. This includes productive, playful, and social applications, as well as the intersection between games and music. The course builds understanding of the limitations and strengths of iterative design and rapid prototyping as research methods, familiarizes students with the theoretical foundations of design exploration, and practices working with physical and digital materials.", "resource_start_char": 0, "resource_end_char": 543, "token_count": 72, "candidates": ["GI_C1_3", "GI_C1_1", "GI_C2_2", "GI_C2_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 17, "end": 34, "quote": "augmented reality"}}]}
{"uv_id": "AI18", "resource_id": "R004", "fragment_id": "F004", "fragment_type": "section", "section_title": "Input/Output Technology activity", "text": "The purpose of this activity is to experiment with ways to do background processing in graphical user interfaces. You will need to look at LongRun.java, which is the Java code you’ll be using. It’s compiled into LongRun.jar. Long Task in Event Handler: Run the Java program. You’ll see a counter, initially at 0. The job of the long task is to increment this counter by 1000 units (and simulating computation or I/O by spending about 1 millisecond to do each increment). So the correct behavior is that you should see the counter rapidly increase from 0 to 1000. Press the Event Handler button. What actually happens? Look at the code. Why does it have the effect you saw? Long Task in Thread: Now click the Thread button, and examine the corresponding code. What happens? Now click the Thread button several times quickly, so that several threads are running at the same time. After all the threads are done, you’d expect to see the counter increase by some multiple of 1000. What actually happens? Why? Using invokeLater(): Now load LongRun.java into Eclipse or an editor, and modify the code for the UsingInvokeLater button so that it calls SwingUtilities.invokeLater() to avoid the problem you saw with the Thread button. There’s an example of the syntax for invokeLater() elsewhere in the program. Click the UsingInvokeLater button a few times quickly to make sure you fixed it, and make sure the counter still increases steadily over time instead of skipping straight to its final value. Using Timers: Now implement the body of the UsingTimer button so that it uses javax.swing.Timer to do the work in the background.", "resource_start_char": 0, "resource_end_char": 1623, "token_count": 273, "candidates": ["GI_C1_3", "GI_C1_1", "GI_C1_4", "GI_C2_1", "GI_C2_2", "GI_C1_2"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 102, "end": 112, "quote": "interfaces"}}]}
{"uv_id": "AI18", "resource_id": "R005", "fragment_id": "F005", "fragment_type": "section", "section_title": "Input activity", "text": "The purpose of this activity is to learn more about how user interfaces handle mouse and keyboard input. You’ll need a modern standards-compliant web browser like Firefox, Opera, or Safari; don’t use an old version of Internet Explorer. The exercises use an in-browser JavaScript environment (Event Workbench). Event Dispatch and Propagation: The workbench is initially configured with an event listener listening for clicks on A. Which objects can you click on to fire this listener? Add a click listener to object C (keeping the listener on A as well). Which listeners fire when you click on C? In what order do they fire? How about when you click on B? Change your listener on C so that it consumes the event by calling event.stopPropagation(). Now which listeners fire when you click on C? How about when you click on B? Finally, change your listener for A from a bubbling-phase listener to a capturing listener. Now which listeners fire when you click on C and in what order? Event Translation: JavaScript’s events for mouse clicking are click, mousedown, mouseup, and dblclick. Attach listeners for these events to one of the objects in the workbench to discover the sequence in which these events occur when the user double-clicks on that object. Which of these events are raw, and which are translated? Mouse Movements: JavaScript’s event for mouse movement is mousemove. Add a mouse move listener to A, and display the mouse coordinates using print(event.clientX + “,” + event.clientY). What origin are these (x,y) positions using? How is this different from the coordinate system used by Java Swing’s mouse events?", "resource_start_char": 0, "resource_end_char": 1624, "token_count": 267, "candidates": ["GI_C1_3", "GI_C1_1", "GI_C2_2", "GI_C2_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 61, "end": 71, "quote": "interfaces"}}]}
{"uv_id": "AI18", "resource_id": "R006", "fragment_id": "F006", "fragment_type": "section", "section_title": "Graphic Design activity", "text": "The purpose of this activity is to learn more about graphic design techniques and concepts: the squint test, simplicity and contrast, visual variables, and Gestalt principles. You’ll need a web browser to do this activity, but it doesn’t particularly matter which browser you use. Information Display: Do a search in Google and look carefully at the list of search results. First, use the squint test to look at the whole page. What parts of the design stand out clearly? Now look closely at the individual search results. Each result consists of several data fields. How is each data field displayed using visual variables? For example, the page-title field uses three variables: hue: blue, size: large font, position: first line of result. Write your findings on a whiteboard. Contrast: Now go to the Drudge Report. Use the squint test to look at the whole page. What parts of the design stand out clearly? What are the major sections of the Drudge Report, and how might you use visual variables to improve their contrast? Alignment: Go to Google Advanced Image Search. How are the form controls aligned vertically? It may help to resize the window so you can watch how the controls move around. What alignment compromises were made, and why? Now look carefully at the horizontal alignment between labels and textboxes. Where are text baselines correctly aligned, and where are they not? It may help to resize the font of your browser so you can see the baselines more clearly.", "resource_start_char": 0, "resource_end_char": 1479, "token_count": 249, "candidates": ["GI_C1_2", "GI_C2_2", "GI_C4_3", "GI_C2_1", "GI_C1_1"], "gold": []}
{"uv_id": "AI18", "resource_id": "R007", "fragment_id": "F007", "fragment_type": "section", "section_title": "Information Visualization activity", "text": "The purpose of this activity is to learn more about information visualization by experimenting with modifications to an existing visualization. You’ll need a web browser. For this activity, we will use IBM’s Many Eyes collaborative visualization system, which allows people to upload data sets, create visualizations of them, and comment on them. Many Eyes was developed by Martin Wattenberg and Fernanda Viegas. Go to the Survival on the Titanic visualization and think about the following questions: Study the visualization for a bit. What does it reveal about patterns of survival among the Titanic’s passengers and crew? How are visual variables being used to display data attributes? What aspects of the Shneiderman mantra (overview, zoom & filter, details on demand) are supported by the visualization? Experiment with assigning different data properties to aspects of the visualization (row, column, size, color). What else can you learn about the patterns of survival?", "resource_start_char": 0, "resource_end_char": 976, "token_count": 149, "candidates": ["GI_C1_1", "GI_C1_4", "GI_C3_3", "GI_C2_1"], "gold": []}
{"uv_id": "AI18", "resource_id": "R008", "fragment_id": "F008", "fragment_type": "section", "section_title": "Color Design and Typography activity", "text": "The purpose of this activity is to explore some of the principles and pitfalls of color design and typography. Judging Color Schemes: Choose a web page whose color scheme you like or dislike. If none come to mind, suggestions include Ask.com or Google Advanced Search. Write down the main foreground and background colors it uses, using simple terms like “dark red” or “pale blue.” Qualitatively speak about the hue, saturation, and value of each one. Judge the legibility of the color scheme. Does foreground text have good contrast with background text? Evaluate it with a squint test. What colors “pop out”? Are these colors used for parts of the page that should attract notice? Evaluate it for color-blind users. Use VisCheck to look at the web page as a deuteranope would see it. How does it fare now for legibility and the squint test? Exploring Typefaces: For this part, you’ll need to use Mozilla Firefox with the Firebug extension installed, so that you can tweak properties of font and spacing dynamically and see their effects. Open up Firebug and use its Inspect feature to locate the paragraph below: “Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men ...”", "resource_start_char": 0, "resource_end_char": 1278, "token_count": 216, "candidates": ["GI_C1_2", "GI_C2_2", "GI_C2_1", "GI_C1_1"], "gold": []}
{"uv_id": "AI18", "resource_id": "R009", "fragment_id": "F009", "fragment_type": "section", "section_title": "Real-world applications of AR and VR", "text": "An augmented reality (AR) overlays digital information onto a user's environment in real time. AR involves hardware such as headsets or smartphones to overlay digital information onto physical environments. Slightly different, virtual reality (VR) is a computer-generated environment that simulates reality and allows users to interact with three-dimensional environments. AR and VR technologies are advancing, particularly in fields like education, health care, and entertainment. New immersive experiences are being created with better hardware, more realistic simulations, and applications in virtual collaboration.", "resource_start_char": 0, "resource_end_char": 618, "token_count": 81, "candidates": ["GI_C1_3", "GI_C4_1", "GI_C1_1", "GI_C4_3"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 3, "end": 20, "quote": "augmented reality"}}]}
{"uv_id": "AI18", "resource_id": "R010", "fragment_id": "F010", "fragment_type": "section", "section_title": "Key Terms: AR and XR", "text": "Augmented reality (AR) is a technology that overlays digital objects or elements onto a real-life picture or scene. Extended reality (XR) refers to the combination of real and virtual environments created by AR, VR, and mixed reality technologies. These immersive technologies are used across education, entertainment and industry to blend the physical and digital worlds and enable interactive learning and collaboration.", "resource_start_char": 0, "resource_end_char": 422, "token_count": 61, "candidates": ["GI_C1_3", "GI_C2_1", "GI_C4_1", "GI_C1_1", "GI_C4_3"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 0, "end": 17, "quote": "Augmented reality"}}]}
{"uv_id": "AI18", "resource_id": "R011", "fragment_id": "F011", "fragment_type": "section", "section_title": "Conceptual questions", "text": "Conceptual questions ask students to describe the differences among virtual reality (VR), augmented reality (AR), and extended reality (XR), and to identify the metaverse and the guidelines for human–computer interaction in immersive environments. The questions challenge learners to explain how AR overlays digital objects onto the real world, how VR creates fully simulated worlds, and how XR encompasses both technologies. Additional questions explore the characteristics of the metaverse, such as convergence of virtual and real environments, and ask students to list human–computer interaction guidelines that apply in immersive experiences.", "resource_start_char": 0, "resource_end_char": 646, "token_count": 89, "candidates": ["GI_C1_3", "GI_C1_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 90, "end": 107, "quote": "augmented reality"}}]}
{"uv_id": "AI18", "resource_id": "R012", "fragment_id": "F012", "fragment_type": "section", "section_title": "Workplace uses of AR", "text": "Augmented reality (AR) is the use of digital objects or elements in a real‑life picture or scene, often viewed through a smartphone or headset. In the workplace, AR technology can overlay diagrams, instructions or data onto physical equipment to assist workers with tasks such as maintenance or training. By integrating computer-generated elements with live camera images, AR enables employees to receive visual guidance without leaving their environment. For example, an AR overlay might show the next step in assembling a machine or highlight components that require inspection. Such workplace applications demonstrate how AR blurs the line between the physical and digital worlds to improve productivity and reduce errors.", "resource_start_char": 0, "resource_end_char": 725, "token_count": 108, "candidates": ["GI_C1_3", "GI_C1_2", "GI_C2_2", "GI_C1_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 0, "end": 17, "quote": "Augmented reality"}}]}
{"uv_id": "AI18", "resource_id": "R013", "fragment_id": "F013", "fragment_type": "section", "section_title": "Immersive technology overview", "text": "Immersive technology facilitates learning in engaging, participatory and exciting ways, utilizing extended reality, virtual reality, simulations, gamification, 360 video and other immersive applications. The page highlights that extended reality (XR), including virtual, augmented, and mixed reality, has emerged as a key alternative education medium alongside traditional online and onsite approaches. It notes that the rise of the metaverse and XR creates new platforms for designing learning experiences that blend physical and digital environments.", "resource_start_char": 0, "resource_end_char": 552, "token_count": 73, "candidates": ["GI_C1_3", "GI_C2_2", "GI_C1_1"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 116, "end": 131, "quote": "virtual reality"}}]}
{"uv_id": "AI18", "resource_id": "R014", "fragment_id": "F014", "fragment_type": "section", "section_title": "Definition of UX design", "text": "User experience design is a user-centered design approach that considers the user's experience when using a product or platform. Research, data analysis and test results drive design decisions rather than aesthetic preferences. Unlike user interface design, which focuses solely on the visual interface, UX design encompasses all aspects of a user's perceived experience, including usability, usefulness, desirability, brand perception and overall performance. It draws from disciplines such as human factors, ergonomics, interaction design, visual design, information architecture and user research. The goal is to create systems that address user needs and provide efficient, delightful experiences.", "resource_start_char": 0, "resource_end_char": 701, "token_count": 95, "candidates": ["GI_C1_3", "GI_C1_2", "GI_C2_2", "GI_C1_1", "GI_C4_2"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 0, "end": 15, "quote": "User experience"}}]}
{"uv_id": "AI18", "resource_id": "R015", "fragment_id": "F015", "fragment_type": "section", "section_title": "Design considerations for AR", "text": "Design considerations for augmented reality include ensuring stable registration of virtual and real objects, achieving legible contrast in varied lighting conditions and minimizing motion-to-photon latency to prevent user disorientation. AR interfaces must provide depth cues and allow glanceable prompts when the user's attention must remain on the primary task, especially in safety-critical situations. Unlike virtual reality, which immerses users in a completely simulated environment, AR overlays digital information onto the real world, meaning that design must account for the context and avoid occluding real-world tasks.", "resource_start_char": 0, "resource_end_char": 630, "token_count": 85, "candidates": ["GI_C1_3", "GI_C1_1", "GI_C1_4"], "gold": [{"competency_id": "GI_C1_3", "evidence": {"start": 26, "end": 43, "quote": "augmented reality"}}]}
