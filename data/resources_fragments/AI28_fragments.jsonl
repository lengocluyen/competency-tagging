{"fragment_id": "F001", "resource_id": "R001", "uv_id": "AI28", "fragment_type": "section", "section_title": "1.1 What Is Data Science?", "start_char": 0, "end_char": 978, "text": "Data science is the interdisciplinary field devoted to collecting, managing and analyzing all types of data to extract meaningful insights.  In its early days data collection, management and analysis were carried out by separate experts in the domain, computer science and statistics.  Technological advances have blurred these boundaries; modern data scientists need expertise across all three domains.  The section introduces the data science cycle – problem definition, data collection, data preparation, analysis and reporting – and notes that data collection and cleaning often consume as much as half the project effort.  Everyday devices like smartphones generate huge quantities of data, and a data scientist or team must design workflows to harness these data ethically and effectively.  The section emphasises that the data science process is iterative and that clear problem definition and careful preparation lay the groundwork for successful analysis and reporting.", "token_count": 142}
{"fragment_id": "F002", "resource_id": "R002", "uv_id": "AI28", "fragment_type": "section", "section_title": "1.2 Data Science in Practice", "start_char": 0, "end_char": 955, "text": "This section demonstrates the breadth of data science applications across business, finance, engineering and the sciences.  It explains how organisations like Walmart and Amazon collect petabytes of customer data and use analytics to forecast demand, optimise inventory and reduce shipping times; for example, Walmart’s predictive analyses led to increased online sales, and Amazon’s regional warehousing reduced delivery times【625203570002831†L99-L137】.  It highlights how data science powers real‑time fraud detection in finance and supports predictive maintenance in engineering.  The section also notes the role of data science in IoT, where sensor networks trigger actions based on collected data, and in weather forecasting, where time‑series analysis and AI improve reliability.  These case studies illustrate the interdisciplinary nature of data science and the importance of big data analytics in modern decision‑making【625203570002831†L99-L139】.", "token_count": 126}
{"fragment_id": "F003", "resource_id": "R003", "uv_id": "AI28", "fragment_type": "section", "section_title": "6.1 What Is Machine Learning?", "start_char": 0, "end_char": 921, "text": "Machine learning is the study of algorithms that learn from data without being explicitly programmed.  The section distinguishes supervised learning, where models are trained on labelled examples, from unsupervised learning, which uncovers hidden structure in unlabelled data, and from reinforcement learning, where agents learn via reward signals.  It outlines the machine‑learning life cycle: formulate the problem, collect and prepare data, engineer features, choose and train a model using training data, validate performance on test or validation sets, and deploy, monitor and refine the system【861774277577375†L96-L147】.  Ethical considerations are also highlighted – models must avoid embedded bias, respect privacy and ensure fairness.  The so‑called Russian tank problem illustrates how biased training data can lead to incorrect classifications and underscores the need for careful data curation and evaluation.", "token_count": 125}
{"fragment_id": "F004", "resource_id": "R004", "uv_id": "AI28", "fragment_type": "section", "section_title": "6.2 Classification Using Machine Learning", "start_char": 0, "end_char": 849, "text": "Classification assigns observations to categories based on input features.  The section uses diagnosing heart disease and classifying music genres as examples of binary and multiclass classification problems【450778709522974†L94-L111】.  Logistic regression is introduced as a binary classifier: it fits a sigmoid function to map continuous inputs to probabilities, and the logit transform linearises the relationship for estimation.  The section also discusses unsupervised classification via clustering; methods such as k‑means and DBScan group data points based on similarity without reference labels【450778709522974†L106-L117】.  Decision trees and random forests are cited as supervised algorithms for multiclass problems.  Readers are encouraged to interpret confusion matrices and measures of accuracy when evaluating classification performance.", "token_count": 106}
{"fragment_id": "F005", "resource_id": "R005", "uv_id": "AI28", "fragment_type": "section", "section_title": "6.3 Machine Learning in Regression Analysis", "start_char": 0, "end_char": 656, "text": "Regression models estimate numeric outputs based on one or more input variables.  This section reviews linear regression and introduces bootstrapping as a resampling method to assess the variability of fitted parameters.  Bootstrapping repeatedly samples from the dataset with replacement, fits a regression model to each sample and records the slope and intercept.  The resulting distributions of parameter estimates help construct confidence intervals and understand how noise in the data affects model parameters【232052914584108†L94-L116】.  The section also previews multiple linear and logistic regression techniques and their implementation in Python.", "token_count": 87}
{"fragment_id": "F006", "resource_id": "R006", "uv_id": "AI28", "fragment_type": "section", "section_title": "6.4 Decision Trees", "start_char": 0, "end_char": 958, "text": "Decision trees are intuitive algorithms that recursively split a dataset into subsets based on feature values.  The section likens a decision tree to the game of 20 Questions and uses a simple tree to classify five objects (tree, dog, human, rock, diamond) by asking yes/no questions【603197268816903†L93-L113】.  Decision trees can solve both classification and regression tasks and are powerful because they handle nonlinear relationships by following different branches.  To build a tree, one measures the information gain from potential splits using entropy; higher information gain indicates a more informative split.  The section introduces information theory concepts such as information, entropy and bits, and explains that decision trees repeatedly partition the data until terminal nodes represent homogeneous subsets【603197268816903†L129-L143】.  Decision trees are popular because they are easy to interpret, though they may overfit if uncontrolled.", "token_count": 132}
{"fragment_id": "F007", "resource_id": "R007", "uv_id": "AI28", "fragment_type": "section", "section_title": "8.1 Ethics in Data Collection", "start_char": 0, "end_char": 781, "text": "Ethical data collection respects autonomy and privacy while complying with legal frameworks.  The section stresses that individuals must be informed about what data are collected and why, and they should have the right to opt out or limit collection【943149028031291†L82-L149】.  Researchers and data scientists should collect only data necessary for the stated purpose (data minimization) and must protect sensitive information.  Regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) mandate transparency about data usage and require organisations to obtain consent and allow deletion requests.  Ethical data collection involves balancing the value of data for innovation with the obligation to safeguard individual rights.", "token_count": 108}
{"fragment_id": "F008", "resource_id": "R008", "uv_id": "AI28", "fragment_type": "section", "section_title": "8.2 Ethics in Data Analysis and Modeling", "start_char": 0, "end_char": 739, "text": "Data analysis and modeling can perpetuate bias if underlying datasets or algorithms are flawed.  This section defines fairness and bias, explaining how historical and social biases in training data can lead to unfair outcomes【25255106691052†L80-L160】.  It cites the COMPAS recidivism algorithm, which predicted higher risk scores for Black defendants than white defendants given similar histories, as an example of algorithmic bias.  The section emphasises anonymization, careful feature selection and regular auditing to mitigate bias, and encourages using techniques such as fairness constraints and reweighting.  It underscores the importance of transparency about model limitations and the ethical imperative to build equitable models.", "token_count": 100}
{"fragment_id": "F009", "resource_id": "R009", "uv_id": "AI28", "fragment_type": "section", "section_title": "8.3 Ethics in Visualization and Reporting", "start_char": 0, "end_char": 643, "text": "Visualisations and reports should convey information honestly and inclusively.  The section advises choosing appropriate chart types and avoiding design choices that misrepresent data, such as distorted scales or  unnecessary 3D effects.  It emphasises providing proper attribution and explaining methods so that audiences can interpret results【19018087781036†L96-L122】.  Ethical reporting means presenting both positive and negative results, acknowledging uncertainty and avoiding sensational headlines.  The section encourages designing visuals that are accessible to people with color‑vision deficiencies and using inclusive colour schemes.", "token_count": 78}
{"fragment_id": "F010", "resource_id": "R010", "uv_id": "AI28", "fragment_type": "section", "section_title": "9.5 Multivariate and Network Data Visualization Using Python", "start_char": 0, "end_char": 652, "text": "This section introduces advanced visualisation techniques for exploring multivariate and network data.  It discusses using scatterplots, heatmaps and 3D graphs to represent relationships among many variables and to identify clusters or trends【503009789950508†L89-L116】.  Examples show how Python libraries such as matplotlib, geopandas and  networkx can visualise large datasets, including geographical heatmaps and social media networks.  The text emphasises that exploring data through multiple visual forms helps reveal patterns before formal analysis, and that 3D visualisations, though harder to interpret, can be useful for high‑dimensional data.", "token_count": 84}
{"fragment_id": "F011", "resource_id": "R011", "uv_id": "AI28", "fragment_type": "section", "section_title": "Classification I: Training & Predicting", "start_char": 0, "end_char": 729, "text": "This chapter introduces classification tasks and the K‑nearest neighbors (KNN) algorithm.  It explains that classification assigns observations to categories, such as distinguishing spam from legitimate email or diagnosing disease based on patient features.  The data are divided into training and test sets; the training set is used to build the model while the test set assesses its performance.  The KNN algorithm classifies a new observation by computing the Euclidean distance to all training points and taking a majority vote among the k closest neighbors【607926398071331†L84-L110】.  Learning objectives include recognising classification problems, computing distances, selecting the parameter k and evaluating predictions.", "token_count": 99}
{"fragment_id": "F012", "resource_id": "R012", "uv_id": "AI28", "fragment_type": "section", "section_title": "Effective Data Visualization", "start_char": 0, "end_char": 625, "text": "Effective visualisation involves matching the plot to the question and audience.  The chapter describes when to use scatter plots (relationships), line plots (trends over time), bar plots (comparisons) and histograms (distributions), and warns against using pie charts or 3D charts because they hinder interpretation【205623987437777†L71-L146】.  It encourages using R’s ggplot2 package to refine visualisations with clear axes, labels and colour schemes.  The chapter emphasises that a good plot should answer a specific question succinctly and invites readers to iterate on their visualisations to improve clarity and impact.", "token_count": 86}
{"fragment_id": "F013", "resource_id": "R013", "uv_id": "AI28", "fragment_type": "section", "section_title": "Regression I: K‑Nearest Neighbors", "start_char": 0, "end_char": 683, "text": "K‑nearest neighbors (KNN) can also be used for regression to predict continuous outcomes.  The chapter describes splitting data into training, validation and test sets, then averaging the responses of the k nearest neighbors to predict a new value【489300390303777†L70-L111】.  It explains the difference between regression and classification – regression predicts numbers rather than categories – and discusses the trade‑off between underfitting and overfitting.  Cross‑validation helps choose the optimal k by minimising the Root Mean Square Prediction Error (RMSPE).  The chapter encourages readers to compare models across different k values to understand bias‑variance trade‑offs.", "token_count": 92}
{"fragment_id": "F014", "resource_id": "R014", "uv_id": "AI28", "fragment_type": "section", "section_title": "Clustering", "start_char": 0, "end_char": 735, "text": "Clustering is an unsupervised learning technique that partitions data into groups without pre‑existing labels.  The chapter introduces the K‑means algorithm, which starts with random cluster centers and alternates between assigning points to the nearest center and updating the centers to the mean of assigned points until convergence.  It emphasises that clustering differs from classification and regression because there is no true label, making evaluation challenging【780836036225500†L96-L132】.  Choosing the number of clusters k is an open question; methods such as the elbow plot or silhouette score can help.  The chapter also highlights how clustering can uncover structure in data, such as grouping animals by physical traits.", "token_count": 104}
{"fragment_id": "F015", "resource_id": "R015", "uv_id": "AI28", "fragment_type": "section", "section_title": "Reading in Data Locally and from the Web", "start_char": 0, "end_char": 770, "text": "Data analysis begins with loading data into the analysis environment.  This chapter explains that reading data means converting files or web resources into R objects such as data frames, and that choosing the right function for the file type reduces later cleaning effort【32232667865968†L72-L87】.  It defines absolute and relative file paths and introduces tidyverse functions like read_csv, read_tsv, read_delim and read_excel for importing plain text and spreadsheet files【32232667865968†L93-L112】.  The chapter’s learning objectives include choosing appropriate read_* functions, skipping header rows, naming columns, and connecting to databases using DBI functions.  It also encourages obtaining data via APIs or web scraping when necessary【32232667865968†L90-L119】.", "token_count": 100}
{"fragment_id": "F016", "resource_id": "R016", "uv_id": "AI28", "fragment_type": "section", "section_title": "Cleaning and Wrangling Data", "start_char": 0, "end_char": 669, "text": "This chapter defines tidy data – a format in which each column represents a variable and each row an observation – and explains why tidiness facilitates analysis.  It describes data frames as tabular structures composed of vectors and lists and defines key terms such as variable, observation and value【867491673740477†L70-L120】.  The learning objectives include using R functions such as pivot_longer, pivot_wider, separate, select, filter, mutate, summarise, map, group_by, across and rowwise to reshape and transform data【867491673740477†L70-L100】.  It emphasises that tidy data reduces complexity, making subsequent modelling and visualisation more straightforward.", "token_count": 89}
{"fragment_id": "F017", "resource_id": "R017", "uv_id": "AI28", "fragment_type": "section", "section_title": "Introduction to Machine Learning (6.036)", "start_char": 0, "end_char": 741, "text": "MIT’s introductory machine‑learning course presents the principles, algorithms and applications of modern machine learning.  It explains how to formulate learning problems, represent data, avoid over‑fitting and ensure generalisation【866217943427263†L66-L78】.  The course covers supervised learning and reinforcement learning and applies these methods to images and temporal sequences.  Learners also study ethical considerations and the importance of proper model evaluation.  Materials on the OCW site are released under a Creative Commons Attribution‑NonCommercial‑ShareAlike licence, allowing reuse and adaptation for non‑commercial purposes as long as attribution is provided and derivatives use the same licence【9455492641263†L51-L78】.", "token_count": 89}
{"fragment_id": "F018", "resource_id": "R018", "uv_id": "AI28", "fragment_type": "section", "section_title": "Machine Learning (6.867)", "start_char": 0, "end_char": 619, "text": "This graduate‑level course surveys a wide range of machine‑learning concepts and algorithms.  Topics include classification, linear regression, boosting, support vector machines, hidden Markov models and Bayesian networks【827173412307365†L134-L147】.  Students learn both the intuitive ideas behind these methods and formal understanding of when and why they work.  Statistical inference is the underlying theme connecting the algorithms.  Course materials are made available under a Creative Commons BY‑NC‑SA licence, allowing non‑commercial reuse with attribution and share‑alike requirements【552894821543995†L51-L78】.", "token_count": 73}
{"fragment_id": "F019", "resource_id": "R019", "uv_id": "AI28", "fragment_type": "section", "section_title": "Advanced Natural Language Processing (6.864)", "start_char": 0, "end_char": 685, "text": "This graduate course introduces natural language processing (NLP) from a computational perspective.  It covers syntactic, semantic and discourse models and emphasises machine‑learning and corpus‑based methods【561087898235713†L113-L124】.  Applications discussed include syntactic parsing, information extraction, statistical machine translation, dialogue systems and summarisation.  As a concentration subject in AI and applications, the course prepares students to build and evaluate NLP systems.  All materials are licensed under the Creative Commons BY‑NC‑SA licence, allowing free access and non‑commercial reuse with attribution and share‑alike provisions【552894821543995†L51-L78】.", "token_count": 78}
