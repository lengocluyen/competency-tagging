{"fragment_id": "F001", "resource_id": "R001", "uv_id": "SR03", "fragment_type": "section", "section_title": "Distributed Computer Systems Engineering (Course Description)", "start_char": 0, "end_char": 424, "text": "This MIT OpenCourseWare course covers abstractions and implementation techniques for designing distributed systems. The description notes that students study server design, network programming, naming and storage systems, and security and fault tolerance. Readings are drawn from current literature. The course emphasizes that distributed systems require careful design to handle concurrency, communication, and reliability.", "token_count": 53}
{"fragment_id": "F002", "resource_id": "R002", "uv_id": "SR03", "fragment_type": "section", "section_title": "Distributed Computer Systems Engineering Lecture Notes", "start_char": 0, "end_char": 528, "text": "The lecture notes page for MIT’s distributed systems course lists a series of handouts and example programs. Topics include concurrency and events, network file systems (NFS), remote procedure calls (RPC), crash recovery, logging, caching, memory management, vector clocks, two‑phase commit, Paxos, and other distributed protocols. Each PDF handout contains lecture slides that explore the theory and practice of building reliable distributed systems, including event‑driven programming, consistency models and scalable storage.", "token_count": 70}
{"fragment_id": "F003", "resource_id": "R003", "uv_id": "SR03", "fragment_type": "section", "section_title": "Modern Distributed Systems", "start_char": 0, "end_char": 812, "text": "The TU Delft course on Modern Distributed Systems introduces the essential functional and non‑functional concerns of distributed computing. The overview explains that distributed systems are the backbone of modern society and discusses cloud, edge and big data processing. Learners explore architectures and techniques to address performance, resilience and scalability. The course highlights challenges such as consistency, availability, elasticity and scalability, and uses current industrial examples to illustrate how cloud and serverless applications, big data and graph processing, interactive games and online services are built. Students are assessed through quizzes, design exercises and a final project in which they design a distributed system, describing its functional and non‑functional properties.", "token_count": 108}
{"fragment_id": "F004", "resource_id": "R004", "uv_id": "SR03", "fragment_type": "section", "section_title": "Fault Tolerance (6.033 Lecture 14 Outline)", "start_char": 0, "end_char": 712, "text": "The outline for Lecture 14 in MIT’s Computer System Engineering course introduces fault‑tolerant design. Students learn to build reliable systems by identifying types of faults and using detection and containment mechanisms. The lecture discusses fail‑fast, fail‑stop and fault masking approaches and describes trade‑offs between replication, performance and cost. Reliability metrics such as mean time to failure, mean time to repair and availability are defined, and the need for replication to improve reliability is highlighted. Examples include redundant arrays of independent disks (RAID 1, RAID 4 and RAID 5), which store data across multiple disks to offer fault tolerance and increased read performance.", "token_count": 102}
{"fragment_id": "F005", "resource_id": "R005", "uv_id": "SR03", "fragment_type": "section", "section_title": "Transactions and Logging (6.033 Lecture 16 Outline)", "start_char": 0, "end_char": 702, "text": "Lecture 16 of MIT’s Computer System Engineering course focuses on transactions and logging. The outline explains how reliable systems use transactions to ensure atomicity and durability. Logging keeps a record of changes with commit and abort records so that the system can recover consistently after failures. The lecture describes how logs are used to implement transactions, including write‑ahead logging and the use of checkpoints to limit recovery work. It also examines performance trade‑offs, caching strategies and the impact of concurrency on transaction processing, emphasising that concurrency control is needed to avoid inconsistent states when multiple transactions execute simultaneously.", "token_count": 98}
{"fragment_id": "F006", "resource_id": "R006", "uv_id": "SR03", "fragment_type": "section", "section_title": "Distributed Transactions and Two‑Phase Commit (6.033 Lecture 18 Outline)", "start_char": 0, "end_char": 768, "text": "Lecture 18 examines distributed transactions and the two‑phase commit protocol. The outline sets up a scenario with a client, a coordinator and several servers that must agree on a transaction outcome. Problems such as network failures and coordinator crashes are described, and the lecture details the two‑phase commit algorithm: in the prepare phase, servers vote on whether they can commit; in the commit phase, they perform the action or abort if any participant voted no. The outline discusses how to handle worker or coordinator failures, logging actions to ensure durability, and the performance costs of waiting for acknowledgments. Students learn that two‑phase commit achieves atomicity in distributed systems but can block progress if the coordinator fails.", "token_count": 116}
{"fragment_id": "F007", "resource_id": "R007", "uv_id": "SR03", "fragment_type": "section", "section_title": "Introduction to Cloud‑Native Applications (Section 12.1)", "start_char": 0, "end_char": 872, "text": "This OpenStax section introduces cloud‑native applications and contrasts monolithic and microservices architectures. It explains that cloud‑native development relies on four key principles—microservices, containerization, continuous delivery and DevOps. Microservices break an application into self‑contained services that can be deployed independently, while monolithic architectures package the user interface, business logic and data access layers into a single codebase. Cloud‑native applications leverage container orchestrators and on‑demand cloud services to provide scalability, automation and rapid elasticity. Cloud‑based applications, by contrast, may migrate a monolithic system to the cloud but still suffer from inflexibility. The section emphasises that microservices and containerization enable applications to scale and evolve without full‑stack downtime.", "token_count": 105}
{"fragment_id": "F008", "resource_id": "R008", "uv_id": "SR03", "fragment_type": "section", "section_title": "Cloud Deployment Technologies (Section 12.2)", "start_char": 0, "end_char": 949, "text": "The deployment technologies section of the OpenStax computer science textbook describes how cloud‑based and cloud‑native applications are deployed. It introduces the major cloud service models—Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS)—and the deployment models (public, private, community and hybrid clouds). The text explains that cloud computing delivers servers, storage and networks over the Internet and notes that compute, storage and network resources can be provisioned on demand. IaaS gives customers control over the infrastructure, while PaaS provides middleware and development tools that reduce operational overhead. SaaS delivers complete applications. The section also contrasts deployment technologies ranging from bare‑metal servers and virtual machines to containers and serverless computing, highlighting how each option balances management complexity, scalability and cost.", "token_count": 126}
{"fragment_id": "F009", "resource_id": "R009", "uv_id": "SR03", "fragment_type": "section", "section_title": "Big Cloud PaaS and IoT (Section 13.3)", "start_char": 0, "end_char": 1032, "text": "Section 13.3 of the OpenStax text discusses Platform as a Service (PaaS) mainstream capabilities and how they support Internet of Things (IoT) applications. It notes that PaaS extends Infrastructure as a Service by providing operating systems and development tools that allow organizations to focus on application development. 5G networks enable mobile edge computing for a variety of IoT devices. IoT network traffic is categorized into telemetry—sensor data sent to servers—and telecommand—commands sent to devices. The section lists application‑layer protocols used in IoT, including MQTT, AMQP and CoAP, because HTTP’s synchronous request‑response model is unsuitable for large sensor networks. A publish/subscribe protocol like MQTT allows many devices to send messages to a broker and subscribe to topics. The text explains that cloud providers such as Microsoft Azure offer IoT PaaS services with SDKs, device management and edge computing, enabling developers to build IoT solutions without maintaining local infrastructure.", "token_count": 147}
{"fragment_id": "F010", "resource_id": "R010", "uv_id": "SR03", "fragment_type": "section", "section_title": "Distributed File Systems (Section 6.5)", "start_char": 0, "end_char": 862, "text": "The file systems chapter introduces distributed file systems (DFS) as file systems that run on multiple servers and present a single namespace to clients. In a DFS, clients read and write remote files as if they were local, and the system hides the physical storage location. Technologies like Google’s GFS, Hadoop’s HDFS and Spark’s Resilient Distributed Datasets (RDDs) are highlighted for processing large datasets. Replication and location transparency improve availability and performance. The section notes that protocols such as remote procedure call (RPC) and distributed hash tables (DHTs) enable communication between nodes, and traditional NFS provides a foundational example of a distributed file system. DFS operations (open, close, read and write) are location transparent, and replication ensures that data remains accessible even when nodes fail.", "token_count": 125}
{"fragment_id": "F011", "resource_id": "R011", "uv_id": "SR03", "fragment_type": "section", "section_title": "Reliability and Security (Section 6.6)", "start_char": 0, "end_char": 795, "text": "In the reliability and security section, the textbook explains that an operating system should deliver services without errors or interruptions. To provide protection, the OS must allow resource sharing, detect and contain accidental errors, and prevent malicious abuse. The protection mechanism has three parts: authentication (verifying user identity), authorization (determining what operations a user may perform) and access enforcement (using authentication and authorization information to control access). The section describes authentication methods such as passwords, badges and two‑factor authentication, and discusses the use of access control lists (ACLs) to manage permissions. It emphasises that even small flaws in authentication or authorization can compromise the entire system.", "token_count": 106}
{"fragment_id": "F012", "resource_id": "R012", "uv_id": "SR03", "fragment_type": "section", "section_title": "Processes and Concurrency (Section 6.3)", "start_char": 0, "end_char": 807, "text": "This section defines a process as an instance of a running program with its own address space, CPU state and resources. The operating system uses processes to execute multiple tasks concurrently rather than waiting for one task to finish before starting another. It introduces the concept of concurrent processing, in which tasks share a processor to improve performance, and explains that modern applications use client–server architectures with inter‑process communication via sockets, remote procedure calls and message passing. The text also describes the process control block (PCB) that stores metadata such as the process ID, state, registers and resource pointers. Concurrency requires careful scheduling, synchronization and isolation to avoid conflicts when multiple processes or threads interact.", "token_count": 115}
{"fragment_id": "F013", "resource_id": "R013", "uv_id": "SR03", "fragment_type": "section", "section_title": "Parallel Programming Models (Section 4.3)", "start_char": 0, "end_char": 860, "text": "The parallel programming models section discusses the move from single‑core to multi‑core processors and defines parallel computing as executing code on multiple processors simultaneously. It distinguishes parallel computing from concurrent computing and distributed computing, noting that parallel computing typically uses shared memory while distributed computing uses message passing across networked machines. The section introduces key terms: a parallel computer is a system with multiple processors; parallel computing is the practice of using such systems; and parallel programming is the technique of dividing a task into parts that run on different processors and synchronizing the results. Examples include multicore CPUs and GPUs used for high‑performance tasks, as well as programming models that allow applications to exploit hardware parallelism.", "token_count": 117}
{"fragment_id": "F014", "resource_id": "R014", "uv_id": "SR03", "fragment_type": "section", "section_title": "Operating Systems Key Terms (Chapter 6)", "start_char": 0, "end_char": 972, "text": "The key terms list for the operating systems chapter provides definitions of concepts used throughout the textbook. Terms relevant to distributed systems include: concurrency – multiple activities happening at the same time; concurrent processing – a computing model in which multiple processors execute instructions simultaneously; distributed file system – a file system distributed across multiple servers that allows network‑wide file sharing; access control list – a list of rules that specify which users are granted access to an object; authentication – verifying a user’s identity; authorization – determining which actions principals can perform; critical section – code that only one thread may execute at a time; deadlock – a situation where threads are blocked forever waiting for each other; fault tolerance – the ability of a system to continue operating when parts fail; replication – creating multiple copies of data to improve reliability and performance.", "token_count": 145}
{"fragment_id": "F015", "resource_id": "R015", "uv_id": "SR03", "fragment_type": "section", "section_title": "Distributed computing (Wikipedia)", "start_char": 0, "end_char": 954, "text": "The Wikipedia article on distributed computing defines the field as the study of computer systems whose components are located on different networked computers. Components communicate and coordinate via message passing to achieve a common goal. The introduction notes that challenges include maintaining concurrency among components, operating without a global clock and tolerating independent failures. Distributed systems cost more to build than monolithic systems but can be more scalable, durable and flexible. The article states that a distributed system typically consists of autonomous nodes with their own local memory that communicate through messages. Properties of distributed systems include failure tolerance, unknown network topology, heterogeneity, dynamic membership and limited individual knowledge. Distributed computing is distinguished from parallel computing in that distributed systems use message passing rather than shared memory.", "token_count": 127}
{"fragment_id": "F016", "resource_id": "R016", "uv_id": "SR03", "fragment_type": "section", "section_title": "Scalability (Wikipedia)", "start_char": 0, "end_char": 1155, "text": "The scalability article describes scalability as the property of a system to handle a growing amount of work by adding resources. For software systems, scalability may be achieved by adding additional servers or processing capacity. The article gives examples from economics and computing and notes that in distributed systems the marginal cost of additional workload should remain nearly constant for the system to be considered scalable. Scalability can be horizontal (adding more nodes) or vertical (adding resources to a single node). Various dimensions of scalability are listed, including administrative scalability (supporting more users or organizations), functional scalability (adding new features), geographic scalability, load scalability (handling heavier workloads), generation scalability (adopting newer components) and heterogeneous scalability (using components from different vendors). The article also distinguishes between scale‑out (horizontal) and scale‑up (vertical) approaches and notes that distributed applications like peer‑to‑peer systems and the Domain Name System demonstrate scalability by spreading load across many nodes.", "token_count": 152}
{"fragment_id": "F017", "resource_id": "R017", "uv_id": "SR03", "fragment_type": "section", "section_title": "Service‑oriented architecture (Wikipedia)", "start_char": 0, "end_char": 801, "text": "The Wikipedia entry on service‑oriented architecture (SOA) describes it as an architectural style in software design that focuses on discrete services rather than a monolithic design. It explains that services provide functionality to other components via network protocols and are intended to be vendor‑ and technology‑independent. A service is defined by four properties: it represents a repeatable business activity with a specific outcome, it is self‑contained, it is a black box for consumers (who do not need to know its internal workings), and it may be composed of other services. SOA facilitates system integration and is common in enterprise applications where services can be reused and orchestrated. Service orientation promotes thinking in terms of services and the outcomes they provide.", "token_count": 120}
